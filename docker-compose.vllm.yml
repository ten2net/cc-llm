version: '3.10'

services:
  vllmapiserver:
    # image: llm-api:vllm
    # image:  cc-aigc/cc-llm:vllm
    image:  cc-aigc/cc-llm:vllm
    command: python api/server.py
    shm_size: 12G
    ulimits:
      stack: 67108864
      memlock: -1
    environment:
      - PORT=8000
      # - MODEL_NAME=qwen
      # - MODEL_PATH=checkpoints/qwen-7b-chat
      # - EMBEDDING_NAME=checkpoints/m3e-base
      # - DEVICE_MAP=auto
      # - DTYPE=half
    volumes:
      - $PWD:/workspace
      # model path need to be specified if not in pwd
#      - /data/checkpoints:/workspace/checkpoints
      - $PWD/../huggingface:/workspace/checkpoints
    env_file:
      # - .env.vllm
      - .env.vllm.4bits
    ports:
      - "13090:8000"
    restart: always
    networks:
      - vllmapinet
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0','1']
              capabilities: [gpu]

networks:
  vllmapinet:
    driver: bridge
    name: vllmapinet